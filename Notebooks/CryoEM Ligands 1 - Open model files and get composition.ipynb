{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iotbx.map_manager import map_manager as MapManager\n",
    "from mmtbx.model import manager as ModelManager\n",
    "from iotbx.data_manager import DataManager\n",
    "import numpy as np\n",
    "from iotbx.map_model_manager import map_model_manager as MapModelManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "error_log_path = \"data/parsing_errors.txt\"\n",
    "\n",
    "maps_and_models_path = \"/net/cci/share/cryoem/maps_and_models/\" # this directory is curated EM structures\n",
    "\n",
    "\n",
    "entry_ids = [entry for entry in os.listdir(maps_and_models_path) if os.path.isdir(maps_and_models_path+entry)]\n",
    "\n",
    "entries = [] # this is a list of group_args objects\n",
    "\n",
    "# read the group_args objects in from the .pkl files in the maps_and_models directory\n",
    "for entry_id in entry_ids:\n",
    "    entry_path = maps_and_models_path+entry_id\n",
    "    filenames = os.listdir(entry_path)\n",
    "    extensions = [filename.split(\".\")[-1] for filename in filenames]\n",
    "    extension_dict = Counter(extensions)\n",
    "    if extension_dict[\"pkl\"] == 1:\n",
    "        pkl_path = entry_path+\"/\"+[filename for filename in filenames if filename.split(\".\")[-1] == \"pkl\"][0]\n",
    "        with open(pkl_path,\"rb\") as fh:\n",
    "            group_args = pickle.load(fh)\n",
    "        group_args.add(key=\"entry\",value=entry)\n",
    "        group_args.add(key=\"entry_pdb\",value=entry[:4])\n",
    "        group_args.add(key=\"entry_emdb\",value=entry[5:])\n",
    "        entries.append(group_args)\n",
    "\n",
    "    else:\n",
    "        errors.append(\"FAILED: \"+entry_path+\": .pkl occurrences != 1\")\n",
    "\n",
    "with open(error_log_path,\"w\") as fh:\n",
    "    for error in errors:\n",
    "        fh.write(error+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entry_composition(group_args):\n",
    "    \"\"\"\n",
    "    1. takes an entry group_args\n",
    "    2. reads it in, gets composition\n",
    "    3. adds composition to group_args\n",
    "    4. returns group_args\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dm = DataManager()\n",
    "        dm.process_model_file(group_args.model_file)\n",
    "        group_args.add(key=\"composition\",value=dm.get_model().composition())\n",
    "    except:\n",
    "        group_args.add(key=\"composition\",value=\"None\")\n",
    "    return group_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract composition in parallel\n",
    "p = Pool(21)\n",
    "results = p.map(extract_entry_composition,entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump list of entries (group_arg objects) to disk\n",
    "with open(\"data/entry_composition.pkl\",\"wb\") as fh:\n",
    "    pickle.dump(results,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it back in\n",
    "entry_composition_path = \"data/entry_composition.pkl\" #list of group_args objects\n",
    "with open(entry_composition_path,\"rb\") as fh:\n",
    "    entry_composition_list = pickle.load(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune for entries without a composition (todo: why do some not have composition?)\n",
    "entries_no_composition = []\n",
    "entries_yes_composition = []\n",
    "ligand_list = []\n",
    "for entry in entry_composition_list:\n",
    "    if entry.composition == \"None\":\n",
    "        entries_no_composition.append(entry)\n",
    "    else:\n",
    "        entries_yes_composition.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write list of entries to disk\n",
    "with open(\"data/entry_composition.pkl\",\"wb\") as fh:\n",
    "    pickle.dump(entries_yes_composition,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phenix",
   "language": "python",
   "name": "phenix"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
